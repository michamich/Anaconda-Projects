{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'Data/train.csv'\n",
    "test_path = 'Data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.iloc[:, 0]\n",
    "X_train = train_df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns input set into monochrome image. Learnt from basic ML algorithm that creating monochrom image helps performance. Original image is retained to compare result afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn result monochrome\n",
    "X_train_mono = X_train.copy()\n",
    "X_test_mono = X_test.copy()\n",
    "\n",
    "X_train_mono[X_train_mono > 1] = 1\n",
    "X_test_mono[X_test_mono >1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick comparison between a black and white and a normal grayscale image visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27e563ce7b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAD9ZJREFUeJzt3X2MFWWWx/HfGQY0yIgYhCDT7cvEbBbHAEqEBJywEZQ1EpxEdIiJGImMZFGIE53Wf/Alq2bDsGt0Q9JEMhAdyfiyijHRbSeoYHxDbUcUW8G4vAZCSMBWIiBn/+hi00s9bd+Xqnurnv5+EnNvH55761T36ePtep6qMncXAKD8ftbsBAAA2aChA0AkaOgAEAkaOgBEgoYOAJGgoQNAJGjoABAJGjoARKKuhm5ms8ysy8y2mVlbVkkBzUZto4ys1jNFzWyQpC8lzZS0S9IHkua5++fZpQc0HrWNsvp5Ha+9XNI2d/9aksxsnaQ5kvosejPjOgPIlbtbBm9DbaNwKqnteg65jJW0s9fXu5IYUHbUNkqpnk/oof9bpD6lmNlCSQvr2A7QaNQ2Sqmehr5LUkuvr38pac+pg9y9XVK7xJ+lKA1qG6VUzyGXDyRdZGYXmNkQSb+TtD6btICmorZRSjV/Qnf342a2WNJrkgZJWu3un2WWGdAk1DbKquZlizVtjD9LkbOMVrlUjdpG3vJe5QIAKBAaOgBEgoYOAJGgoQNAJGjoABAJGjoARIKGDgCRoKEDQCRo6AAQCRo6AESChg4AkaChA0AkaOgAEAkaOgBEgoYOAJGgoQNAJGjoABAJGjoARIKGDgCRqPkm0ZJkZt9I+lbSj5KOu/ukLJICmo3aRhnV1dAT/+TuBzJ4H6BoqG2UCodcACAS9TZ0l/TfZvahmS3MIiGgIKhtlE69h1ymuvseMxslqcPMvnD3t3oPSH4Z+IVA2VDbKB1z92zeyOx+Sd3uvvwnxmSzMaAP7m5Zvye1jSKopLZr/oRuZmdI+pm7f5s8v0rSg7W+X1lk9T/AsjPLvG8WxkCt7XPOOScYv+OOO1KxadOmBcdOnz69om0dP348GH/llVdSsS+++CI4tqurq6JtSdKLL76YinV3d1eVWxnUc8hltKT/Sn6xfy7pL+7+aiZZAc1FbaOUam7o7v61pPEZ5gIUArWNsmLZIgBEgoYOAJHIbJVLRRsrwEoAJjUbr5ETqHmscqlEEWr73HPPTcWuvfba4Njrr78+FZsxY0bF2zp69GgwvmfPnopeP2jQoGC8paWl4hzq1dnZGYyvXbs2FXviiSeCYxs5gVpJbfMJHQAiQUMHgEjQ0AEgEjR0AIgEDR0AIpHF9dBRpUpXfbAiB9UInTY/fnzl50e9/PLLwfimTZtSsfXr1wfHVno6/pQpU4LxN954IxW78847g2Pff//9irYlSZMnT07F5s2bFxy7YsWKVGz06NHBsffee2/FOTQCn9ABIBI0dACIBA0dACJBQweASHDqf5UafBp7xWPzyiuv+sgx3wF76v9NN92Uio0cOTI4NjSBum3btsxz6susWbOC8VC+Tz31VC45DBs2LBjfsmVLKnb48OHg2MsuuywVO3bsWH2J9YFT/wFgAKGhA0AkaOgAEAkaOgBEot+GbmarzWy/mW3pFTvbzDrM7KvkcUS+aQLZo7YRm35XuZjZbyR1S1rr7r9OYv8m6aC7P2pmbZJGuPsf+91YAVYCoPGXFCjqDS6o7fhceumlwXjoNP/bbrstOPbMM89Mxa688srg2A0bNlSRXX0yWeXi7m9JOnhKeI6kNcnzNZKuqzo7oMmobcSm1mPoo919ryQlj6OySwloKmobpZX71RbNbKGkhXlvB2g0ahtFU+sn9H1mNkaSksf9fQ1093Z3n+Tuk2rcFtBI1DZKq9ZP6OslzZf0aPL4UmYZoV9FvU56Iyc/c0RtN9Fpp52Wit11113BsQsWLEjFLrzwwuDY7777LhX7+OOPg2Nnz56dih06dCg4tmgqWbb4jKR3JP2Dme0yswXqKfaZZvaVpJnJ10CpUNuITb+f0N09fFsPKbyOBygJahux4UxRAIgEDR0AIkFDB4BI5L4OvQyKumqkCCJZuTJgnX766cF4aIXI4MGD697e3r17U7ExY8akYi0tLcHXh1aYtLa2Bse+9tprqdjtt98eHNvZ2ZmKHThwIDi2zPiEDgCRoKEDQCRo6AAQCRo6AESCSdHIMak5sM2cOTMYD51Of8EFF+Sdzv/ZuXNnMP7II4+kYn1dc7yrqyvTnGLAJ3QAiAQNHQAiQUMHgEjQ0AEgEv3eJDrTjZXsRroxnEE60CZFq7lJdJbKVttDhw5NxUaNyudue7feemsqNnfu3ODY0NmbixcvDo795JNP6kusZDK5STQAoBxo6AAQCRo6AESChg4AkajknqKrzWy/mW3pFbvfzHabWWfy3zX5pglkj9pGbPpd5WJmv5HULWmtu/86id0vqdvdl1e1sZKtBGi2Rq+yiWFFTDWrXKjt5hkyZEgwvmjRolSsra0tOPadd95JxW688cbg2GPHjlWRXTFlssrF3d+SdDCTjIACobYRm3qOoS82s78nf7aOyCwjoPmobZRSrQ19paRfSZogaa+kP/U10MwWmtlmM9tc47aARqK2UVo1NXR33+fuP7r7CUmrJF3+E2Pb3X2Su0+qNUmgUahtlFlN10M3szHufvJusL+VtOWnxjdDNROKRZ0MrCavLCZQQ+9R1O9NXspQ2+PHjw/GQ9cYP3iwmFMER48eDcYfe+yxVCx0M2hJ6ujoSMXefffd4NgbbrghFdu+fftPpVhK/TZ0M3tG0nRJI81sl6Rlkqab2QRJLukbSb/PMUcgF9Q2YtNvQ3f3eYHwkznkAjQUtY3YcKYoAESChg4AkaChA0AkorjBRb37EPNKjrx+vkX9nsV2g4vQTSe2bAkvvJk+fXoq9vnnn2edUmFMmTIlFVu1alVw7PDhw1OxGTNmBMd++eWX9SWWE25wAQADCA0dACJBQweASNDQASASA25StKiTeY0U80RpbJOit9xySyp2xRVXBMcuWLAgjxRKpbW1NRgPXT5g9+7dwbGzZ89OxY4cOVJfYhlgUhQABhAaOgBEgoYOAJGgoQNAJGjoABCJmm5wgXLrazVKI1c8oXaHDh1qdgqFtWPHjmB82bJlqdi6deuCY6dOnZqKvf766/Ul1iB8QgeASNDQASASNHQAiES/Dd3MWsxsg5ltNbPPzGxJEj/bzDrM7KvkcUT+6QLZobYRm35P/TezMZLGuPtHZvYLSR9Kuk7SLZIOuvujZtYmaYS7/7Gf92r6qf8hRThlvQhi+D5Wc+p/GWr76quvTsUeeuih4NiZM2emYkyg9hgyZEgq1tnZGRz75ptvpmKLFi3KPKdqZXLqv7vvdfePkuffStoqaaykOZLWJMPWqOcXASgNahuxqeoYupmdL2mipPckjXb3vVLPL4ak9K1VgJKgthGDitehm9kwSc9LWuruhyv989rMFkpaWFt6QP6obcSiok/oZjZYPQX/tLu/kIT3JccgTx6L3B96rbu3u/skd5+URcJAlqhtxKSSVS4m6UlJW919Ra9/Wi9pfvJ8vqSXsk8PyA+1jdhUssplmqSNkj6VdCIJ36eeY41/ldQqaYekue5+sJ/3KuQql74UYdVGs5Xt5iFVrnIpfG0PHTo0Fdu+fXtw7JIlS1Kx5557Ljj2xIkTwfhAsmHDhmD8rLPOSsUmTpyYdzr9qqS2+z2G7u6bJPX1RldWmxRQFNQ2YsOZogAQCRo6AESChg4AkYjieuihybgsJkpD71GEib+8cD304vn+++9TsXvuuSc4du3atanYxRdfHBz78MMPp2I//PBDldmVx913352KjR8/Pjj2wQcfzDud3PAJHQAiQUMHgEjQ0AEgEjR0AIgEDR0AItHvqf+Zbiyn06Or0eD9bdi2+jIA97cpSRShtm+++eZUrL29PTi2q6srFWtrawuO3bhxYyrW3d1dZXbZGzduXCrW140oQvHly5cHxz7wwAOp2JEjR6rMLnuZ3OACAFAONHQAiAQNHQAiQUMHgEgMuEnREE5571sRJjqrMZAnRUMmTJgQjC9dujQVmzx5cnDs8OHDU7FXX301OPbZZ59NxUITiq2trcHXT506NRW76qqrgmPHjh2bim3bti049vHHH0/FVq5cGRxbVEyKAsAAQkMHgEjQ0AEgEpXcJLrFzDaY2VYz+8zMliTx+81st5l1Jv9dk3+6QHaobcSmkuuhH5f0B3f/yMx+IelDM+tI/u3f3T18uhVQfNQ2olL1Khcze0nSE5KmSuqupuiLuhKgGjGviCnbipaQela5DPTaPuOMM4Lx0A01pk2bFhx7ySWXpGKhm3Scd955wdeHLjOwadOm4Ni33347Fevo6AiMlI4ePRqMl0nmq1zM7HxJEyW9l4QWm9nfzWy1mY2oOkOgIKhtxKDihm5mwyQ9L2mpux+WtFLSryRNkLRX0p/6eN1CM9tsZpszyBfIHLWNWFTU0M1ssHoK/ml3f0GS3H2fu//o7ickrZJ0eei17t7u7pPcfVJWSQNZobYRk0pWuZikJyVtdfcVveJjeg37raQt2acH5IfaRmz6nRQ1s2mSNkr6VNKJJHyfpHnq+ZPUJX0j6ffuvref9yr9xBGKrZpJUWobZVJJbXMtF0SFa7kgVlzLBQAGEBo6AESChg4AkaChA0AkaOgAEAkaOgBEgoYOAJGgoQNAJGjoABCJSm5wkaUDkv4neT4y+To27FfzhC+y3Rgna7sM36daxbpvZdivimq7oaf+/78Nm22O8Sp17NfAFvP3KdZ9i2m/OOQCAJGgoQNAJJrZ0NubuO08sV8DW8zfp1j3LZr9atoxdABAtjjkAgCRaHhDN7NZZtZlZtvMrK3R289Sckf4/Wa2pVfsbDPrMLOvksfS3THezFrMbIOZbTWzz8xsSRIv/b7lKZbapq7Lt28nNbShm9kgSf8p6Z8ljZM0z8zGNTKHjP1Z0qxTYm2S/ubuF0n6W/J12RyX9Ad3/0dJUyT9S/JzimHfchFZbf9Z1HUpNfoT+uWStrn71+5+VNI6SXManENm3P0tSQdPCc+RtCZ5vkbSdQ1NKgPuvtfdP0qefytpq6SximDfchRNbVPX5du3kxrd0MdK2tnr611JLCajT95QOHkc1eR86mJm50uaKOk9RbZvGYu9tqP62cda141u6KGbnLLMpqDMbJik5yUtdffDzc6n4Kjtkoi5rhvd0HdJaun19S8l7WlwDnnbZ2ZjJCl53N/kfGpiZoPVU/RPu/sLSTiKfctJ7LUdxc8+9rpudEP/QNJFZnaBmQ2R9DtJ6xucQ97WS5qfPJ8v6aUm5lITMzNJT0ra6u4rev1T6fctR7HXdul/9gOhrht+YpGZXSPpPyQNkrTa3f+1oQlkyMyekTRdPVdr2ydpmaQXJf1VUqukHZLmuvupE0yFZmbTJG2U9KmkE0n4PvUcbyz1vuUpltqmrsu3bydxpigARIIzRQEgEjR0AIgEDR0AIkFDB4BI0NABIBI0dACIBA0dACJBQweASPwvvwFkpUkSbKEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e78399780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "i = 7\n",
    "\n",
    "img_mono = X_train_mono.iloc[i, :].values.astype('float32').reshape(28, 28)\n",
    "img = X_train.iloc[i, :].values.astype('float32').reshape(28,28)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_mono, cmap = 'gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train = X_train.values.astype('float32').reshape(X_train.shape[0], 28, 28, 1)\n",
    "    X_train_mono = X_train_mono.values.astype('float32').reshape(X_train_mono.shape[0], 28, 28, 1)\n",
    "except:\n",
    "    print('converted and reshaped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing grayscale image through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanx = X_train.mean().astype(np.float32)\n",
    "stdx = X_train.std().astype(np.float32)\n",
    "\n",
    "def standardization(x):\n",
    "    return (x - meanx)/stdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels needs to be converted into categorical in order to be usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "num_classes\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random seed in order to confirm repeatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses 3 layers. \n",
    "1. Normalization layer\n",
    "2. Flattening layer\n",
    "3. Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear model imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Lambda, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization, Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(standardization, input_shape = (28, 28, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adagrad\n",
    "model.compile(optimizer = RMSprop(lr = 0.001),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "gen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_norm, X_val_norm, y_train_norm, y_val_norm = train_test_split(X_train, y_train, test_size = 0.10, random_state = 42)\n",
    "batches = gen.flow(X_train_norm, y_train_norm, batch_size = 64)\n",
    "val_batches = gen.flow(X_val_norm, y_val_norm, batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Michael\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., 37800, validation_data=<keras.pre..., epochs=1, validation_steps=4200)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37800/37800 [==============================] - 221s 6ms/step - loss: 0.2403 - acc: 0.9342 - val_loss: 0.3272 - val_acc: 0.9102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(batches, batches.n, \n",
    "                              nb_epoch = 1, \n",
    "                              validation_data = val_batches, \n",
    "                              nb_val_samples = val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mono = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mono, X_val_mono, y_train_mono, y_val_mono = train_test_split(X_train_mono, \n",
    "                                                                      y_train, \n",
    "                                                                      test_size = 0.10, \n",
    "                                                                      random_state = 42)\n",
    "\n",
    "batches_mono = gen_mono.flow(X_train_mono, y_train_mono, batch_size = 64)\n",
    "val_batches_mono= gen_mono.flow(X_val_mono, y_val_mono, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      "    1/37800 [..............................] - ETA: 2:05:04 - loss: 3.0906 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Michael\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., 37800, validation_data=<keras.pre..., epochs=1, validation_steps=4200)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37800/37800 [==============================] - 202s 5ms/step - loss: 2.0424 - acc: 0.3635 - val_loss: 1.8485 - val_acc: 0.4692\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(batches_mono, batches_mono.n, \n",
    "                              nb_epoch = 1, \n",
    "                              validation_data = val_batches_mono,\n",
    "                              nb_val_samples = val_batches_mono.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
